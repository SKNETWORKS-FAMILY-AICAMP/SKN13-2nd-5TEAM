import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, precision_recall_curve
)
from imblearn.over_sampling import SMOTE, RandomOverSampler
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st

# 1. ì‚¬ìš©ì ê¸°ë°˜ ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜
def prepare_data(df):
    import numpy as np
    from sklearn.model_selection import train_test_split
    from imblearn.over_sampling import SMOTE, RandomOverSampler

    base_cols = ["steps", "calories", "very_active_minutes", "moderately_active_minutes", "distance"]
    df = df.dropna(subset=base_cols + ["id"])

    df_user = df.groupby("id")[base_cols].mean().reset_index()

    # ğŸ¯ ì•ˆì „í•œ ì ìˆ˜ ê³„ì‚° ë°©ì‹ (Series ê¸°ë°˜)
    score = (
        (df_user["steps"] < 6400).astype(int) +
        (df_user["calories"] < 1800).astype(int) +
        (df_user["very_active_minutes"] < 6).astype(int) * 2 +
        (df_user["moderately_active_minutes"] < 8).astype(int) +
        (df_user["distance"] < 4600).astype(int)
    )

    df_user["CHURNED"] = (score >= 4).astype(int)

    churned_df = df_user[df_user["CHURNED"] == 1]
    nonchurn_df_pool = df_user[df_user["CHURNED"] == 0]

    if len(churned_df) < 2 or len(nonchurn_df_pool) < 2:
        raise ValueError("CHURNED ë˜ëŠ” ë¹„ì´íƒˆì ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")

    if len(nonchurn_df_pool) < len(churned_df):
        nonchurn_df = nonchurn_df_pool.sample(n=len(churned_df), replace=True, random_state=42)
    else:
        nonchurn_df = nonchurn_df_pool.sample(n=len(churned_df), replace=False, random_state=42)

    df_balanced = pd.concat([churned_df, nonchurn_df]).sample(frac=1, random_state=42)

    X = df_balanced.drop(columns=["CHURNED", "id"])
    y = df_balanced["CHURNED"].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

    if y_train.value_counts().min() < 6:
        sampler = RandomOverSampler(random_state=42)
    else:
        sampler = SMOTE(random_state=42)

    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)
    return X_train_res, X_test, y_train_res, y_test


# 2. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (Precision ê¸°ì¤€ íŠœë‹)
def train_best_xgb_model(X_train, y_train):
    param_grid = {
        "max_depth": [3, 4],
        "learning_rate": [0.01, 0.05],
        "n_estimators": [100, 200]
    }

    grid = GridSearchCV(
        XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
        param_grid=param_grid,
        scoring="precision",
        cv=3,
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    return grid.best_estimator_

# 3. ìµœì  threshold (Precision ìš°ì„ )
def find_best_threshold_by_precision(model, X_test, y_test):
    y_proba = model.predict_proba(X_test)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)
    candidates = [(p, r, t) for p, r, t in zip(precisions, recalls, thresholds) if p >= 0.75]
    if candidates:
        best = max(candidates, key=lambda x: x[1])  # recall ìµœëŒ€
        return best[2], best[0], best[1]
    else:
        idx = (precisions * recalls).argmax()
        return thresholds[idx], precisions[idx], recalls[idx]

# 4. í‰ê°€ í•¨ìˆ˜

def evaluate_model(model, X_test, y_test, threshold):
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)

    report = {
        "accuracy": round(accuracy_score(y_test, y_pred), 4),
        "precision": round(precision_score(y_test, y_pred, zero_division=0), 4),
        "recall": round(recall_score(y_test, y_pred, zero_division=0), 4),
        "f1_score": round(f1_score(y_test, y_pred, zero_division=0), 4),
        "threshold": float(round(threshold, 3))
    }

    if report["accuracy"] == 1.0 or report["f1_score"] == 1.0:
        st.warning("âš ï¸ ëª¨ë¸ ì„±ëŠ¥ì´ ë„ˆë¬´ ì™„ë²½í•©ë‹ˆë‹¤. ë°ì´í„° ëˆ„ìˆ˜ë‚˜ ê¸°ì¤€ ê³¼ë‹¨ìˆœ ê°€ëŠ¥ì„± í™•ì¸ í•„ìš”!")

    st.caption(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(y_test)}")
    st.caption(f"CHURN ë¶„í¬ (y_test): {y_test.value_counts().to_dict()}")
    st.caption(f"ì˜ˆì¸¡ê°’ ë¶„í¬: {dict(zip(*np.unique(y_pred, return_counts=True)))}")

    return report

# 5. í‰ê°€ ì§€í‘œ ë°” ì°¨íŠ¸

def plot_metrics_bar(report_summary):
    metrics = ["accuracy", "precision", "recall", "f1_score"]
    values = [report_summary[m] for m in metrics]

    plt.figure(figsize=(6, 4))
    sns.barplot(x=metrics, y=values)
    plt.ylim(0, 1)
    plt.title("ëª¨ë¸ í‰ê°€ ì§€í‘œ")
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, str(v), ha='center')
    st.pyplot(plt)

# 6. í†µí•© ì‹¤í–‰

def train_xgb_model_with_smote(df):
    X_train, X_test, y_train, y_test = prepare_data(df)
    model = train_best_xgb_model(X_train, y_train)
    threshold, _, _ = find_best_threshold_by_precision(model, X_test, y_test)
    report = evaluate_model(model, X_test, y_test, threshold)
    return model, report, threshold
